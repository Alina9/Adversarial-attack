# –êdversarial attack
The implementation of Targeted fast gradient sign method (T-FGSM).
## Dataset
MNIST: handwritten digits

## Method description

An adversarial attack involves modifying the original image in such a way that the changes are nearly imperceptible to the human eye. The classifier mistakenly classifies the image, while the original image is classified correctly.

Here's an example where altering the images leads to them being mistakenly classified as eights.
